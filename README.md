# ğŸ§  LLM Cerveau Network

**Un cerveau collaboratif composÃ© d'agents spÃ©cialisÃ©s utilisant plusieurs modÃ¨les LLM quantifiÃ©s, Ã©changeant et raisonnant ensemble via AutoGen et Ollama.**

---

## ğŸš€ Objectif du projet

Ce projet expÃ©rimental vise Ã  crÃ©er une **intelligence artificielle distribuÃ©e** oÃ¹ chaque modÃ¨le de langage (LLM) joue le rÃ´le d'un neurone spÃ©cialisÃ©. Les modÃ¨les communiquent entre eux afin de gÃ©nÃ©rer collectivement des rÃ©ponses cohÃ©rentes et intelligentes sur divers sujets, avec une latence optimisÃ©e pour Ãªtre utilisable sur des machines modestes (ex: PC portable GTX 1650 4GB).

---

## âš™ï¸ Architecture technique
- **Orchestrateur** : [AutoGen](https://github.com/microsoft/autogen)
- **ModÃ¨les locaux** : Ollama (modÃ¨les quantifiÃ©s GGUF 4-bit)
- **API REST** : FastAPI
- **MÃ©moire vectorielle** : ChromaDB
- **Interface utilisateur (future)** : Streamlit
- **Conteneurisation** : Docker et Docker Compose
- **DÃ©veloppement** : Ubuntu + VSCode

---

## ğŸ“Œ SpÃ©cifications matÃ©rielles minimales
- **CPU** : Ryzen 5 (ou similaire)
- **GPU** : NVIDIA GTX 1650 4GB (ou supÃ©rieure)
- **RAM** : 16 Go (idÃ©alement 24 Go ou plus)
- **Stockage** : SSD 512 Go minimum

---

## ğŸš€ Installation rapide

Clonez le dÃ©pÃ´t et installez les dÃ©pendances :

```bash
git clone https://github.com/1BoNoBo1/LLM-Cerveau-Network.git
cd LLM-Cerveau-Network
chmod +x scripts/setup.sh
./scripts/setup.sh
```

---

## ğŸ§© RÃ´les des Agents (neurones LLM)

| Agent            | RÃ´le                          | ModÃ¨le recommandÃ© (GTX 1650) |
|------------------|--------------------------------------|-----------------------------|
| **Initiateur**   | Lance la rÃ©flexion initiale  | Gemma 2B (quantifiÃ©) |
| **CrÃ©atif**      | GÃ©nÃ¨re des idÃ©es originales | Phi-2 (quantifiÃ©) |
| **Critique**     | Analyse et critique les rÃ©ponses | Phi-2 (quantifiÃ©) |
| **SynthÃ©tiseur** | Fusionne les meilleures rÃ©ponses | Gemma 2B (quantifiÃ©) |

---

## ğŸ”„ Protocole dâ€™interaction entre agents
- Lâ€™utilisateur dÃ©clenche une session via lâ€™API FastAPI.
- Les agents interagissent sÃ©quentiellement avec une limite de temps initiale de **2 minutes** (ajustable).
- Les Ã©changes sont structurÃ©s en JSON (format modulable).

---

## ğŸ“ Structure du dÃ©pÃ´t
```
LLM-Cerveau-Network/
â”œâ”€â”€ agents/              # Agents spÃ©cialisÃ©s
â”œâ”€â”€ api/                 # API FastAPI
â”œâ”€â”€ docker/              # Fichiers Docker
â”œâ”€â”€ models/              # Stockage des modÃ¨les LLM quantifiÃ©s
â”œâ”€â”€ data/                # Base de donnÃ©es vectorielle
â”œâ”€â”€ scripts/             # Scripts setup et lancement rapide
â”œâ”€â”€ tests/               # Tests unitaires
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt     # DÃ©pendances Python
â””â”€â”€ README.md            # Documentation
```

---

## ğŸš€ Ã‰volutions prÃ©vues
- IntÃ©gration interface Streamlit
- Optimisation pour utilisation GPU/Cloud (RunPod, RTX 4090, A100)
- Ajout d'agents multimodaux

---

## ğŸ“– Contribution
Toute contribution est bienvenue ! Suivez ces Ã©tapes :
1. Forkez le projet
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b fonctionnalitÃ©`)
3. Proposez une Pull Request claire et documentÃ©e

---

## âš ï¸ SÃ©curitÃ© et confidentialitÃ©
- Aucun modÃ¨le n'est exposÃ© directement sur internet.
- Les clÃ©s API ou informations sensibles ne doivent jamais Ãªtre versionnÃ©es.

---

## ğŸ“š Licence
Ce projet est sous licence MIT. Libre d'Ãªtre partagÃ©, modifiÃ©, utilisÃ© pour lâ€™Ã©ducation et la communautÃ©.

